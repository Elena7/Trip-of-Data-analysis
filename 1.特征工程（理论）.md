# 1.数据预处理
- **数据预览**
```python
.info() #查看多种信息：总行数和列数、每列元素类型和non-NaN的个数，总内存
.ndim, .shape, .size #查看维数，形状，元素个数。
.head(), .tail() #查看头5行和后5行
.describe() #快速查看每一列的统计信息，默认排除所有NaN元素。
.columns() #查看列名
.isnull.sum(),.notnull.sum() #查看缺失值总数
(df.shape[0]-df.count())/df.shape[0] #查看每个特征的缺失率
(df.shape[1]-df.T.count())/df.shape[1] #查看每个个体所缺失的特征个数
### 使用missingno库
import missingno as msno
missingValueColumns = merged.columns[merged.isnull().any()].tolist()
# bar图缺失值可视化分析
msno.bar(merged[missingValueColumns],figsize(20,8),color="#34495e",fontsize=12,labels=True)
# matrix密集图查看数据完整性
msno.matrix(merged[missingValueColumns],width_ratios=(10,1),figsize=(20,8),color=(0,0, 0),fontsize=12,sparkline=True,labels=True)
# heatmap图查看相关性
msno.heatmap(merged[missingValueColumns],figsize=(20,20))
# dendrogram树状图查看相关性
msno.dendrogram(collisions)
### 使用pandas_profiling库预览数据
import pandas_profiling
pandas_profiling.ProfileReport(df)
```
- **缺失值**
  - **不处理缺失值**：如LightGBM和XGBoost均可自行处理缺失值
  - **dropna**: 删除实例/特征
  - **fillna**: 
  1.填充固定值 fillna('-99')
  2.均值 fillna(data['灰度分'].mean())
  3.众数 fillna(data['灰度分'].mode())
  4.前一个数 fillna(method='pad')
  5.后一个数 fillna(method='bfill')
  6.插值法填充 interpolate(inplace=True)
  7.KNN填充
  ```python
  from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute
  dataset = KNN(k=3).complete(dataset)
  ```
  8.random forest填充
  ```python
  from sklearn.ensemble import RandomForestRegressor
  ```
  9.使用fancyimpute包中的其他方法填充
- **异常值**
  - 数值型单变量：极差，四分位数间距，均差，标准差等
  - 异常点：
   1.距离：离(曼哈顿距离)、欧氏距离和马氏距离等
   2.密度
- **重复值**
  - 考虑是否去重
 data.duplicated() 和 data.drop_duplicates()，前者标记出哪些是重复的（true），后者直接将重复删除；也可以根据单变量剔除重复，data.drop_duplicates(['Area'])
- **特征转换**
  - **连续型**
  1. 标准化：转化后的数据符合标准正态分布
  ```python
from sklearn.preprocessing import StandardScaler
#标准化，返回值为标准化后的数据
StandardScaler().fit_transform(iris.data)
```
  2. 归一化
  ```python
from sklearn.preprocessing import Normalizer
#归一化，返回值为归一化后的数据
Normalizer().fit_transform(iris.data)
```
  3. 区间缩放：压缩范围，具体问题具体分析
  ```python
from sklearn.preprocessing import MinMaxScaler
#区间缩放，返回值为缩放到[0, 1]区间的数据
MinMaxScaler().fit_transform(iris.data)
```
  4. 二值化
  ```python
from sklearn.preprocessing import Binarizer
 #二值化，阈值设置为0.5，返回值为二值化后的数据
Binarizer(threshold=0.5).fit_transform(iris.data)
```
 5. 散化分箱处理：减少噪声干扰，避免过拟合，增加鲁棒性
```python
#等距分组
#等量分组
#决策树离散化（最优分组）
#卡方分箱
```
 6. 不处理
 根据模型类型而定
 
  - **离散型**
 1. 数值化处理
 2. one-hot
 `from sklearn.preprocessing import OneHotEncode`
 3. 顺序性哑变量
  - **时间序列**
  时间戳通常需要分离成多个维度比如年、月、日、小时、分钟、秒钟、时区
  - **文本**
 1. 去除空白 strip()
 2. 分列 split()
 3. 创建成哑变量
  - **不均衡样本**
 - 过采样
 - 欠采样

# 2.特征选择
- **特征检验**
 -  **单变量**：正态性检验、显著性分析
 -  **多变量**：一致性检验、多重共线性
- **过滤式选择特征**
 - 方差选择
 ```python
from sklearn.feature_selection import VarianceThreshold
#方差选择法，返回值为特征选择后的数据
#参数threshold为方差的阈值
VarianceThreshold(threshold=3).fit_transform(iris.data)
```
 - 皮尔逊相关系数法
 ```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr
#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
#参数k为选择的特征个数
SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```
 - 互信息法
 ```python
from sklearn.feature_selection import SelectKBest
from minepy import MINE
#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
def mic(x, y):
    m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)
#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```
 - 卡方检验
 ```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2 
#选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
```
 - F检验
 ```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
SelectKBest(score_func=f_classif, k=4).fit_transform(iris.data, iris.target)
```
- **封装式选择特征**
 - 完全搜索[穷举]：递归消除特征
 ```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```
 - 启发式搜索[贪心]
 - 随机搜索[策略+好运气]
- **嵌入式选择特征**
 - 基于惩罚项
 ```python
#使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征：
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
#带L1惩罚项的逻辑回归作为基模型的特征选择
SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target)
#!!!
#优化：L1惩罚项降维，结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：
from sklearn.linear_model import LogisticRegression
class LR(LogisticRegression):
    def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0,
                 fit_intercept=True, intercept_scaling=1, class_weight=None,
                 random_state=None, solver='liblinear', max_iter=100,
                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):
        #权值相近的阈值
        self.threshold = threshold
        LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C,
                 fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight,
                 random_state=random_state, solver=solver, max_iter=max_iter,
                 multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)
        #使用同样的参数创建L2逻辑回归
        self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)
    def fit(self, X, y, sample_weight=None):
        #训练L1逻辑回归
        super(LR, self).fit(X, y, sample_weight=sample_weight)
        self.coef_old_ = self.coef_.copy()
        #训练L2逻辑回归
        self.l2.fit(X, y, sample_weight=sample_weight)
        cntOfRow, cntOfCol = self.coef_.shape
        #权值系数矩阵的行数对应目标值的种类数目
        for i in range(cntOfRow):
            for j in range(cntOfCol):
                coef = self.coef_[i][j]
                #L1逻辑回归的权值系数不为0
                if coef != 0:
                    idx = [j]
                    #对应在L2逻辑回归中的权值系数
                    coef1 = self.l2.coef_[i][j]
                    for k in range(cntOfCol):
                        coef2 = self.l2.coef_[i][k]
                        #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0
                        if abs(coef1-coef2) < self.threshold and j != k and self.coef_[i][k] == 0:
                            idx.append(k)
                    #计算这一类特征的权值系数均值
                    mean = coef / len(idx)
                    self.coef_[i][idx] = mean
        return self
#使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：
from sklearn.feature_selection import SelectFromModel
#带L1和L2惩罚项的逻辑回归作为基模型的特征选择
#参数threshold为权值系数之差的阈值
SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)
```
 - 基于树模型
```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import GradientBoostingClassifier
#GBDT作为基模型的特征选择
SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)
```
 - 基于深度学习
 在特征学习中，K-means算法可以将一些没有标签的输入数据进行聚类，然后使用每个类别的质心来生成新的特征。
- 附录：
1. 过滤式方法运用统计指标来为每个特征打分并筛选特征，其聚焦于数据本身的特点。其优点是计算快，不依赖于具体的模型。缺点是选择的统计指标不是为特定模型定制的，因而最后的准确率可能不高。而且因为进行的是单变量统计检验，没有考虑特征间的相互关系，并且不能对交互项进行选择。
2. 包裹式方法使用模型来筛选特征，通过不断地增加或删除特征，在验证集上测试模型准确率，寻找最优的特征子集。包裹式方法因为有模型的直接参与，因而通常准确性较高，但是因为每变动一个特征都要重新训练模型，因而计算开销大，其另一个缺点是容易过拟合。
3. 嵌入式方法利用了模型本身的特性，将特征选择嵌入到模型的构建过程中。典型的如 Lasso 和树模型等。准确率较高，计算复杂度介于过滤式和包裹式方法之间，但缺点是只有部分模型有这个功能。

# 3.构造及提取
- **构造方法**
四则运算、特征交叉、分解类别特征、重构数值量、分解Datatime、窗口变量统计、基于机器学习等
- **特征提取**
 - 线性降维
  1. 基于L1惩罚项
  2. 主成分分析法（PCA）
  ```python
from sklearn.decomposition import PCA
#主成分分析法，返回降维后的数据
#参数n_components为主成分数目
PCA(n_components=2).fit_transform(iris.data)
```
  3. 线性判别分析法（LDA）
```python
from sklearn.lda import LDA
#线性判别分析法，返回降维后的数据
#参数n_components为降维后的维数
 LDA(n_components=2).fit_transform(iris.data, iris.target)
```
 - 非线性降维
  1. 核主成分分析(KPCA):带核函数的PCA
  2. 局部线性嵌入（LLE）
  3. 拉普拉斯特征映射（LE）
  4. 随机邻域嵌入（SNE）
  5. t-分布邻域嵌入（T-SNE）
 - 迁移学习降维
 1. 迁移成分分析(TCA)
